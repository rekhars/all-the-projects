{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c7b3825-1d60-479a-8a64-1b2887ff8dae",
    "_uuid": "a82e2ad6a19c9ce75530f0b3a3c4a21c8c82a006"
   },
   "source": [
    "# DonorsChoose EDA Challenge\n",
    "This notebook is focused on understanding the metadata and text-based influences of whether teachers' resource proposals are approved by DonorsChoose.org. We start with individuals factors, then integrate these in ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2031b0a5-e375-48e5-b734-54c285fdf216",
    "_uuid": "da074a2cbd664dc164f7d904f16cfb7aeca334fe"
   },
   "source": [
    "# Contents\n",
    "1. Initial Data Wrangling\n",
    "2. Feature Analysis\n",
    "    *     2.1. Project Categories\n",
    "    *     2.2. Number of Previously Posted Projects\n",
    "    *     2.3. Funding and Grade Category\n",
    "    *     2.4. Number Submitted by State\n",
    "    *     2.5. Year and Month\n",
    "    *     2.6. Number of Completed Essay Questions\n",
    "3. Text and Feature Engineering\n",
    "    * 3.1. Approach\n",
    "    * 3.2. Tf-idf Vectorization with Bag of Words and N-grams\n",
    "    * 3.3. Feature Reduction\n",
    "    * 3.4. Classification using LightGBM\n",
    "4.  Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b19a0e22-beb7-41fa-9f5b-a64bb7098731",
    "_uuid": "66c4d0182b4651c30127ff56914a01ecdcc808db"
   },
   "source": [
    "# 1. Data Wrangling\n",
    "* There are 1081830 projects within the training dataset. \n",
    "* Many teachers submitted multiple project needs with the same project ID. \n",
    "* Collapsing and aggregating within ID, there were 182,080 separate Project ID's available for training.\n",
    "* Requested funds were totaled by project ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b5cd3565-e188-4c8e-b8a3-6c9541fb50d4",
    "_uuid": "9c1f85dad98e52882b726bcfd378976b499e7958",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as gbm\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "import time\n",
    "import gc #garbage collection\n",
    "pd.set_option('display.max_columns', 100)\n",
    "#pd.set_option('max_colwidth',40)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "66af8e31-e768-4d96-8bae-eb83c2fadaef",
    "_uuid": "cc1034005947438d2bdca33bd8a63fc920bfae45",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create and wrangle columns in training or test data\n",
    "def wrangle(data, resource_data):\n",
    "    \n",
    "    #Get year, month from submitted datetime\n",
    "    data['project_submitted_datetime'] = pd.to_datetime(data['project_submitted_datetime'])\n",
    "    data['Year'] = data['project_submitted_datetime'].dt.year\n",
    "    data['Month'] = data['project_submitted_datetime'].dt.month\n",
    "    \n",
    "    #Merge with resource file\n",
    "    data = data.merge(resource_data, on='id')\n",
    "    \n",
    "    #Find number and amount of simultaneously submitted projects with same ID; aggregate $total\n",
    "    data['$total'] = data['price'] * data['quantity']\n",
    "    newcol = data.groupby('id')['$total'].agg({'$aggregated':'sum', 'number_submitted':'count'})\n",
    "    if 'project_is_approved' in data.columns:\n",
    "        newcol['number_approved'] = data.groupby('id')['project_is_approved'].sum()\n",
    "    newcol = newcol.reset_index()    \n",
    "    \n",
    "    #Drop duplicated ID rows, keeping only the first one; merge the new columns\n",
    "    data.drop_duplicates(['id'], keep='first', inplace=True)\n",
    "    data = data.merge(newcol, on='id', how='inner')    \n",
    "    \n",
    "    #Compute categorical project aggregated funding requests\n",
    "    data['$Total_cat'] = pd.cut(data['$aggregated'], bins=[0,100,250,500,1000,16000], \n",
    "                                labels=['0-100 USD','101-250 USD','251-500 USD','501-1000 USD','>1000 USD'])\n",
    "    data['#Prior_cat'] = pd.cut(data[\"teacher_number_of_previously_posted_projects\"],bins=[-1,1,5,10,25,50,500],\n",
    "                                         labels=['0-1','2-5','6-10','11-25','26-50','51+'])\n",
    "    #Separate project categories and tabulate--there are maximum 3 categories (by prior analysis)\n",
    "    data[['cat1','cat2','cat3']] = data['project_subject_categories'].str.split(',', 3, expand=True)\n",
    "    data['cat1'] = data['cat1'].str.strip()\n",
    "    data['cat2'] = data['cat2'].str.strip()\n",
    "    data['cat3'] = data['cat3'].str.strip()\n",
    "    #Get the number of tags that were assigned to each project\n",
    "    data['#Project_categories'] = data[['cat1','cat2','cat3']].count('columns')\n",
    "    data['#Project_essays'] = data[['project_essay_1','project_essay_2','project_essay_3','project_essay_4']].count('columns')\n",
    "    data['essays'] = data['project_essay_1'].astype(str)+' '+data['project_essay_2'].astype(str)\n",
    "    data.drop(columns=['teacher_id','project_submitted_datetime','project_title','project_essay_3',\n",
    "                      'project_essay_4', 'description','quantity','price','$total'], inplace=True)\n",
    "    data['teacher_prefix'].fillna('unknown', inplace=True)\n",
    "    data.rename({'teacher_prefix': 'Teacher_prefix', 'school_state':'State',\n",
    "                 'project_grade_category':'Grade_cat',\n",
    "                 'project_subject_categories':'Subject_cat'}, axis='columns', inplace=True)\n",
    "    gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "0ae64b8a-3a75-4fc2-812c-629807e2b049",
    "_uuid": "48f0d8d9e5912a8e22d1e4667591516b5234070f"
   },
   "outputs": [],
   "source": [
    "#Read Training Data, Test Data, and Resources Data\n",
    "resources = pd.read_csv('../input/resources.csv', sep=',')\n",
    "train_data = pd.read_csv('../input/train.csv', sep=',')\n",
    "train_data = wrangle(train_data, resources)\n",
    "print(train_data.shape,'\\n',train_data.columns)\n",
    "display(train_data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "554b21b7-1840-4892-9ac8-fa7afb65811b",
    "_uuid": "72164b04d9800b3e3b50329b7c058ee0019a1959"
   },
   "source": [
    "# 2. Feature Analysis\n",
    "## 2.1. Approvals by Project Categories\n",
    "* Multiple project category keywords are listed for many projects. To use this as a feature, we will  disaggregate these into separate columns in both training and test data, splitting on commas We'll do the same for both training and test dataframes. In addition, we'll add columns to count the number of filled-in project essays, in both training and test datasets.\n",
    "* In the training data, there were a total of 1,480,886 project category tags, split into 9 categories. Next we gather all tags, and count/plot by projects that contain that tag (as the first tag). When the category, 'Warmth,' appeared as *first* category tag, 85% of its projects were funded, compared to only 24% of 'Special Needs' projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "46e7b47f-82a3-4d22-8a88-0c589fb8e31b",
    "_uuid": "f281456ed9064837f49d17136cc5f6518dd042dc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = pd.DataFrame(train_data[['cat1','cat2','cat3']].stack().value_counts(), columns=['#TotalTags'])\n",
    "categories['#approved'] = train_data.groupby('cat1')['project_is_approved'].sum()\n",
    "categories['%approved'] = round(100 * categories['#approved']/categories['#TotalTags'], 2)\n",
    "categories = categories.sort_values(by='%approved', ascending=False)\n",
    "\n",
    "ax = categories[['#TotalTags','#approved']].sort_values(by='#TotalTags', ascending=False).plot(kind='bar', legend=True, fontsize=16, \n",
    "                                        figsize=(12,6), rot=30, title='Funding Counts by First Project Category')\n",
    "ax.set_xlabel('Project Category', fontsize=14)\n",
    "ax.set_ylabel('#Projects with Tag', fontsize=16)\n",
    "print(categories.sum(), '\\n', categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28f9b760-42d4-418a-a689-baa173c035e5",
    "_uuid": "ccc6f556d5d1b6dc3f6528d2d1dbeedf4448d271"
   },
   "source": [
    "## 2.2. Approvals by Number of Previously Posted Projects\n",
    "Perhaps those previously submitting greater numbers of projects are more successful in getting their projects approved. The prior number of previously submitted projects by a submitter varied from 0 to 451. There was quite a long right tail to the distribution, as shown in the plot (which was cutoff at 20 prior projects). To transform these into a useful feature, we can bin these into categories, defined by [0,1,2,3,4+] prior projects. Also, it could be interesting to determine if a teacher's chance of project approval improves with each submitted project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5137389-ffc0-4eea-a848-b29535b72ced",
    "_uuid": "b3da87d53cd7a200b3af7b43bb287f5eb54e6fcc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior = pd.DataFrame(train_data.groupby('#Prior_cat')['project_is_approved'].count())\n",
    "prior['#approved'] = train_data.groupby('#Prior_cat')['project_is_approved'].sum()\n",
    "prior.rename(columns = {'project_is_approved': '#total'}, inplace=True)\n",
    "prior['#not_approved'] = prior['#total'] - prior['#approved']\n",
    "prior['%_approved'] = 100 * prior['#approved']/prior['#total']\n",
    "\n",
    "#fig, axes = plt.subplots(1,1, figsize=(16,8), sharex=True)\n",
    "axA = prior[['#total','#approved','#not_approved']].plot(kind='bar', figsize=(12,6), \n",
    "                            legend=True, fontsize=16, title='Number of Previously Posted Projects')\n",
    "axB = prior['%_approved'].plot(kind='line', secondary_y=True, fontsize=14, color='r', legend=True, alpha=1.0)\n",
    "axA.set_xlabel('Number of Prior Projects Posted by Teacher', fontsize=16)\n",
    "axA.set_ylabel('Number of Submitted Projects', fontsize=16)\n",
    "axB.set_ylabel('Percentage Projects Approved', fontsize=16)\n",
    "axB.set_ylim(70,100)\n",
    "print(train_data[\"teacher_number_of_previously_posted_projects\"].describe()[['min','50%','mean','max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "47a32cd8-3698-4d46-aea3-ed82aa4b12d8",
    "_uuid": "813763e2b3cf3fb2bfd67b3d531f0c8f1cb6d82f"
   },
   "source": [
    "## 2.3. Approvals and Funding by Grade Category\n",
    "We can see that approximately 83-85% of projects were approved in the training data; these were weakly based on grade category, with Grades 3-5 slightly higher (85.4%) than Grades 9-12 (83.5%). However, this seems to be a pretty weak factor. A clear trend did emerge, however: while the number of projects submitted and approved declined by a factor of 4 from PreK-2 to grades 9-12, the average project size increased by ~50% over these same intervals. Also, the mean *approved* project $ amounts were about one-third larger than the mean *non-approved* projects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "966cee5e-99a1-401b-bc5a-921618d7607b",
    "_uuid": "0b8f36b5746c69eb48cb3284209be7325942449d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grades = pd.DataFrame(train_data.groupby('Grade_cat')['project_is_approved'].count())\n",
    "grades['#approved'] = train_data.groupby('Grade_cat')['project_is_approved'].sum()\n",
    "grades['$approved'] = round(train_data[train_data['project_is_approved']==1].groupby('Grade_cat')['$aggregated'].mean(),2)\n",
    "grades.rename(columns = {'project_is_approved': '#total'}, inplace=True)\n",
    "grades['#not_approved'] = grades['#total'] - grades['#approved']\n",
    "grades['$not_approved'] = round(train_data[train_data['project_is_approved']==0].groupby('Grade_cat')['$aggregated'].mean(),2)\n",
    "grades['%approved'] = round(100 * grades['#approved']/grades['#total'],1)\n",
    "grades = grades.reindex(index=['Grades PreK-2','Grades 3-5','Grades 6-8','Grades 9-12'])\n",
    "display(grades)\n",
    "\n",
    "#Plot the dataframe\n",
    "ax1 = grades[['#total', '#approved', '#not_approved']].plot(kind='bar', figsize=(12,6), rot=0, legend=True,\n",
    "                                                               fontsize=14, color=['gray','g','r'], alpha=0.5,\n",
    "                                                               title='Project Approval Counts by Grades')\n",
    "ax2 = grades['%approved'].plot(kind='line', secondary_y=True, fontsize=14, legend=True, alpha=0.8)\n",
    "ax1.set_xlabel('Grade Category', fontsize=16)\n",
    "ax1.set_ylabel('Number of Projects', fontsize=16)\n",
    "ax1.set_ylim(0,80000)\n",
    "ax2.set_ylabel('Percentage Projects Approved', fontsize=16)\n",
    "ax2.set_ylim(74,90)\n",
    "\n",
    "ax3 = grades[['$approved', '$not_approved']].plot(kind='bar', figsize=(12,6), rot=0, color=['g','r'], alpha=0.5,\n",
    "                                                     fontsize=14, title='Mean Project Amount by Grades')\n",
    "ax3.set_xlabel('Grade Category', fontsize=16)\n",
    "ax3.set_ylabel('Mean (Total) Project Amounts (USD)', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8370cfa2-b6f8-4dda-b73d-c7f4944ae7ef",
    "_uuid": "dc3dc3159fbad0eeee403e2c8cafe787b9cc1a57"
   },
   "source": [
    "## 2.4. Project Approvals and Mean Funding by State\n",
    "Perhaps some state have a better chance of getting their projects funded. Because there are dramatic differences in number of projects among states, we will use percentage project approval by state. From the following training data analysis, all states had 7d0-90% of their projects funded. The plot limits the x-axis range to 80-90, to make it easier to spot differences between states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "99164eba-974e-436d-9f38-c4d202c60e55",
    "_uuid": "a0aabf85f0d7c5da0a7643838b2c872fd516fd09",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = pd.DataFrame(train_data.groupby('State').size(), columns=['#submitted'])\n",
    "states['#approved'] = train_data.groupby('State')['project_is_approved'].sum()\n",
    "states['%_approved'] = round(100 * states['#approved']/states['#submitted'], 2)\n",
    "states['$_approved'] = train_data[train_data['project_is_approved']==1].groupby('State')['$aggregated'].mean()\n",
    "states.sort_values(by='%_approved', ascending=True, inplace=True)\n",
    "fig, axes = plt.subplots(1,1, figsize=(16,8), sharex=True)\n",
    "states['%_approved'].plot(kind='line',color=['blue'], alpha=1.0, legend=True)\n",
    "states['$_approved'].plot(kind='bar', color=['gray'], secondary_y=True, ylim=(0,160), alpha=0.7, linewidth=9, legend=True)\n",
    "axes.set_xlabel('Projects from State', fontsize=16)\n",
    "axes.set_ylim(70,90)\n",
    "axes.set_ylabel('Percentage of Projects Approved', fontsize=16)\n",
    "axes.right_ax.set_ylabel('Mean Approved Project Value (USD)', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "33cf12da-5734-4de8-a96a-8b895edfa766",
    "_uuid": "bb441e8965dee55813c2035ddddf28b5b7406719"
   },
   "source": [
    "## 2.5. Project Submission by Year, and Month of Year\n",
    "Within this dataset, project submissions were only from two years: 2016 and 2017. Although more than twice as many projects were submitted in 2016, a slightly greater percentage (81 vs 79%) were approved in 2017. The most popular month for project submissions is August (start of school year) , followed by September. The least number of submissions were in May and June, at the end of the school year. Despite large differences in number of submitted projects over the year, the percentage of approved projects didn't depend on this timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "82f41586-090e-4af0-b54a-2b98401adb8a",
    "_uuid": "03fc01055b108f135b6f39dc19bfdcfd58ad1fb6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = pd.DataFrame(train_data.groupby('Year').size(), columns=['#_submitted'])\n",
    "years['#_approved'] = train_data.groupby('Year')['project_is_approved'].sum()\n",
    "years['%_approved'] = round(100 * years['#_approved']/years['#_submitted'], 2)\n",
    "years.sort_values(by='%_approved', ascending=True, inplace=True)\n",
    "print(years)\n",
    "\n",
    "month_names = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "months = pd.DataFrame(train_data.groupby('Month').size(), columns=['#_submitted'])\n",
    "months['#_approved'] = train_data.groupby('Month')['project_is_approved'].sum()\n",
    "months['%_approved'] = 100 * months['#_approved']/months['#_submitted']\n",
    "months.sort_index(ascending=True, inplace=True)\n",
    "print(months)\n",
    "axm1 = months[['#_submitted', '#_approved']].plot(kind='bar', figsize=(12,6), \n",
    "                                            rot=0, legend=True, sharex=True, alpha=0.7,\n",
    "                                            fontsize=16, title='Project Approvals by Month')\n",
    "axm2 = months['%_approved'].plot(secondary_y=True, legend=True, color='g', linewidth=3)\n",
    "axm1.set_xlabel('Month', fontsize=16)\n",
    "axm1.set_xticklabels(month_names)\n",
    "axm1.set_ylabel('Number of Projects', fontsize=16)\n",
    "axm2.set_ylabel('Percentage of Projects Approved', fontsize=16)\n",
    "axm2.set_ylim(0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c773b17-a06b-432f-846e-df30e686e1c4",
    "_uuid": "eb8f3dc0617fb753c06b3ce3f6f95f35f4a51ba6"
   },
   "source": [
    "## 2.6. Project Approvals by Number of Completed Essay Questions\n",
    "As a quick check, most teachers only filled out the first 2 of the 4 essay questions. Of those who filled out all 4, the percentage approved was slightly higher than those who filled out 2 (87% vs 85%). We will focus on analyzing the first 2 essay questions, plus the resource summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "082e7cf6-56c7-4c36-8de3-6c1fa4e8c8bf",
    "_uuid": "10322a11ccd93c1276b992e09f10be5fd97b248b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essays = pd.DataFrame(train_data.groupby('#Project_essays').size(), \n",
    "                      columns=['#_completed_essay_questions'])\n",
    "essays['#_approved'] = train_data.groupby('#Project_essays')['project_is_approved'].sum()\n",
    "essays['%_approved'] = round(100 * essays['#_approved']/essays['#_completed_essay_questions'],1)\n",
    "essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "08ebd332-54c6-484d-ae93-97c7772d9bc5",
    "_uuid": "be8c052960292db8396881f0264e1a17367b1fc9"
   },
   "source": [
    "## 2.7. Number of Submissions per ID\n",
    "It could be useful to compare the number of *prior* submissions per teacher ID, versus the number of *current* submissions per teacher ID in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6506d912-6f54-4641-834e-3f85692decb2",
    "_uuid": "d621f85c162733df8efd274bd824eccc4fbb414a",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_data.groupby('id')['id'].count().value_counts(ascending=False)\n",
    "submissions = pd.DataFrame()\n",
    "submissions['Number prior submitted per teacher'] = train_data['teacher_number_of_previously_posted_projects'].value_counts(ascending=False)\n",
    "submissions['Currently submitted per teacher'] = train_data['number_submitted'].value_counts(ascending=False)\n",
    "submissions\n",
    "ax = submissions.plot(kind='bar', figsize=(14,6))\n",
    "ax.set_xlabel('Number of Submissions by Teacher', fontsize=16)\n",
    "ax.set_ylabel('Count', fontsize=16)\n",
    "ax.set(xlim=(0,100), ylim=(0,50000))\n",
    "for label in ax.xaxis.get_ticklabels()[::2]:\n",
    "    label.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8d61b11a-ea3d-4518-941f-67471908addd",
    "_uuid": "7afe5a9f09487751bac77f5e47f1af9d96442c58"
   },
   "source": [
    "## 2.8. Approvals by Teacher Prefix\n",
    "Presuming that head of school (e.g., 'Dr.') may have greater success than others in getting projects approved. The data doesn't strongly support this notion, with approval percentages varying from about 80%-85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25f8ab57-f3a3-41ee-8fa4-059ac0bfd2c2",
    "_uuid": "b4adb4b3a29697a55180f3a23dcd186c887ca07d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = pd.DataFrame(train_data.groupby('Teacher_prefix').size(), columns=['#_prefix'])\n",
    "prefix['#_approved'] = train_data.groupby('Teacher_prefix')['project_is_approved'].sum()\n",
    "prefix['%_approved'] = round(100 * prefix['#_approved']/prefix['#_prefix'], 1)\n",
    "display(prefix)\n",
    "ax = prefix[['#_prefix','#_approved']].plot(kind='bar', rot=0, figsize=(12,6))\n",
    "ax.set_xlabel('Teacher Prefix', fontsize=16)\n",
    "ax.set_ylabel('Count', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a323cffc-7dac-4f06-855d-cce7768ff425",
    "_uuid": "a0160b1ee74fb30390584e7d3218585e34cba24b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Release some memory\n",
    "dfs = [grades, states, years, months, essays, submissions, prefix]\n",
    "del dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10fc29e6-e6e5-4479-ba6c-34b8efca89fb",
    "_uuid": "6dcf8a90414537b8487666417c739358c5524bcc"
   },
   "source": [
    "# 3. Text Analysis: Word Features\n",
    "## 3.1. Approach\n",
    "Regarding the project essays, teachers responded to the following essay prompts:\n",
    "1. Open with the challenge facing your students (`project_essay_1`)\n",
    "2. Tell us more about your students (`project_essay_2`)\n",
    "3. Inspire your potential donors with an overview of the resources you're requesting (`project_essay_3`)\n",
    "4. Close by sharing why your project is so important (`project_essay_4`) \n",
    "\n",
    "All teachers responded to the title, resource summary, and the first two essay questions. Perhaps project success is related to particular words stated as primary challenges that students face. So, we can look at the most frequent words from {Project Essay 1, 2, and Project Resource Summary}, and build a vocabulary, and relate these to project funding success.\n",
    "\n",
    "A generalized pipeline was created using the following:\n",
    "* Transforming 3 text columns into sparse (#samples, #features) tf-idf-normalized matrices\n",
    "* Removing stop words\n",
    "* Limiting each text matrix to 100 top features\n",
    "* Label Encoding 4 categorical columns\n",
    "* Combining all 7 columns into a unified sparse matrix\n",
    "    * Project Resource Summary (text)\n",
    "    * Project Essay 1 (text)\n",
    "    * Project Essay 2 (text)\n",
    "    * Month of year (1-12)\n",
    "    * Grade category (PreK-2, 3-5, 6-8, 9-12)\n",
    "    * Number of Teacher Prior Submited Projects cateogry (0-1, 2-5, 6-10, 11-25, 26-50, 51+)\n",
    "    * Primary Subject Area Category (first one listed)\n",
    "* Defining training, testing splits from the training data (test = 20%)\n",
    "* Fitting a Gradient Boosted Decision Tree classifier to the transformed training data\n",
    "* Using the classifier to predict project approval from test data\n",
    "* Computing the AUC from actual vs. predicted test data\n",
    "* Computing the proportion of test data projects that were correctly predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7484ecb6-eb1e-4997-a866-5f3ed6997bc0",
    "_uuid": "025af957b4894fa3b2a364b05586f3883c39abe8"
   },
   "source": [
    "## 3.2. Tokenization: Tf-idf Word Vectorization and Categorical Label Encoding\n",
    "The aforementioned text columns were encoded into for subsequent classifier training. Several options were selected via extensive hyperparameter tuning:\n",
    "* The text was encoded into N-grams (unigrams and bigrams), to capture some context for Bag of Words features. \n",
    "* Either a Count or Tf-idf vectorizer can be selected; tf-idf was used here. \n",
    "* Stop words were removed\n",
    "* Feature scaling (0-1) could optionally be used if required by a classifier (SVM, Logistic Regression, etc.)\n",
    "* Output was limited to a defined number of features per text column (e.g., 1000), to enable reasonable classification performance. This was accomplished through the vectorizer, although feature reduction could optionally be accomplished (although much more slowly) via SVD, PCA, or NMF functions within sklearn. \n",
    "* l2 normalization was used for the vectorizer\n",
    "* Non-text, categorical features were label-encoded and converted to sparse (csr) matrices\n",
    "* Feature reduction was performed to select the most predictive features, using an ANOVA F_classif variance approach\n",
    "* The text and non-text feature sets were joined into a single large sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "fe36bbea-caf3-460c-9d54-e4cf4a672cf6",
    "_uuid": "17f552d7923cbfadab2aff83b4be35e337c2c6ae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize text column features using tfidf/count vectorizer\n",
    "def vect_text(data, n_resource_feat, n_essay_feat, scaling=False):\n",
    "    #n_resource_feat = desired number of text features from resource descriptions\n",
    "    #n_essay_feat = desired number of text features from essay quesstions\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    #import nltk\n",
    "    #stemmer = nltk.stem.PorterStemmer()\n",
    "    if scaling:\n",
    "        from sklearn.preprocessing import MaxAbsScaler\n",
    "        scaler = MaxAbsScaler(copy=False) #Works with sparse matrices\n",
    "    t0 = time.time()\n",
    "    #use_idf: False for CountVectorizer\n",
    "    #min_df: minimum document frequency to be included\n",
    "    #stop_words: common words that won't be included\n",
    "    #ngram_range: specify (min, max) n-gram size; #features explode for larger sizes\n",
    "    #norm: normalization using either 'l1', 'l2', or None (for Count)\n",
    "    \n",
    "    #Vectorize project resource summary using tfidf or count\n",
    "    vect = TfidfVectorizer(use_idf=True, norm='l2', min_df=5, ngram_range=(1,2), \n",
    "                                   max_features=n_resource_feat, stop_words=\"english\")\n",
    "    \n",
    "    #Word stemming (optional--resource intensive)\n",
    "    #tokens = stemmer.stem(token for token in data['project_resource_summary'][:nrec])\n",
    "    #X_resource = vect.fit_transform(tokens)\n",
    "    \n",
    "    X_resource = vect.fit_transform(data['project_resource_summary'])\n",
    "    if scaling:\n",
    "        X_resource = scaler.fit_transform(X_resource)\n",
    "    resource_names = vect.get_feature_names()\n",
    "    \n",
    "    #Vectorize project essays\n",
    "    vect = TfidfVectorizer(use_idf=True, norm='l2', min_df=5, ngram_range=(1,2), \n",
    "                                   max_features=n_essay_feat, stop_words=\"english\")    \n",
    "    X_essay = vect.fit_transform(data['essays'])\n",
    "    if scaling:\n",
    "        X_essay = scaler.fit_transform(X_essay)\n",
    "    essay_names = vect.get_feature_names()\n",
    "\n",
    "    #Combine into a sparse matrix\n",
    "    X_vect = hstack([X_resource, X_essay], 'csr')\n",
    "    feature_names = resource_names + essay_names\n",
    "    print('\\nText Features:')\n",
    "    print('   Vectorization time:', round(time.time() - t0,1))\n",
    "    print('   Shape:', X_vect.shape)\n",
    "    return X_vect, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "87541804-d037-4b12-bd45-8c94c5eb1bbc",
    "_uuid": "6f64d461800f25bbc44c5217744195459f83ba3a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize categorical features\n",
    "#Note: not necessary if assigning categorical features in LightGBM classifier\n",
    "def vect_cat(data, features, scaling=False):\n",
    "    t0 = time.time()\n",
    "    if scaling:\n",
    "        from sklearn.preprocessing import MaxAbsScaler\n",
    "        scaler = MaxAbsScaler(copy=False) #Works with sparse matrices\n",
    "        \n",
    "    le = LabelEncoder()\n",
    "    feat_vect = []\n",
    "    for f in range(len(features)):\n",
    "        feat = le.fit_transform(data[features[f]].astype(str))  \n",
    "        if scaling:\n",
    "            feat = scaler.fit_transform(feat.reshape(-1,1))\n",
    "        feat_vect.append(csr_matrix(feat).T)\n",
    "    X_vect = hstack(feat_vect, 'csr')\n",
    "    print('\\nCategorical Features:')\n",
    "    print('   Vectorization time:', round(time.time() - t0,1))\n",
    "    print('   Shape:', X_vect.shape)\n",
    "    return X_vect, feat_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "aa84698d-7576-41f2-86db-ae9581b29a58",
    "_uuid": "31c591ad56c8cdd652357b46cf981f2795ca8f5b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reduce categorical features only for training; no labels for prediction\n",
    "def feat_reduce(X_vect, features, labels, filt_value):\n",
    "    #filt_value = Percent desired reduction in number of categorical features\n",
    "    from sklearn.feature_selection import SelectPercentile\n",
    "    #Categorical feature reduction\n",
    "    t0 = time.time()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vect, labels, test_size=0.2, random_state=0)\n",
    "    select = SelectPercentile(percentile=filt_value)\n",
    "    select.fit(X_train, y_train)\n",
    "    feature_mask = np.array(select.get_support())\n",
    "    selected_features = np.array(features)[feature_mask]\n",
    "    X_vect = X_vect[:,feature_mask]\n",
    "    X_train_selected = select.transform(X_train)\n",
    "    print('\\nCategorical Feature Reduction:')\n",
    "    print('   Reduction time:', round(time.time() - t0,1))\n",
    "    print('   Final Shape:', X_vect.shape)\n",
    "    print('   Selected Features:', selected_features)\n",
    "    return X_vect, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "364c58e4-1aa6-4639-b21c-a8f6f3df0bc1",
    "_uuid": "99db1ef9eea9285bb3a4a5aa3e6a44fd93aec6ea"
   },
   "outputs": [],
   "source": [
    "nrec = len(train_data) #number of records to use with random sampling\n",
    "nres = 100             #Number of top Project Resource text features to use\n",
    "ness = 200             #Number of top Essay text features to use\n",
    "#pcat = 100              #Percentage of top categorical features to use; 100 for no feature reduction\n",
    "#Input categorical features to consider using; final set depends on outcome of feature reduction\n",
    "cat_features = ['Month','Grade_cat', '#Prior_cat','Subject_cat', \n",
    "                '$Total_cat', 'Teacher_prefix','#Project_categories']\n",
    "\n",
    "#Get reduced dataset, using sampling, for development\n",
    "#train_data_reduce = train_data.sample(nrec)\n",
    "\n",
    "X_text, text_features = vect_text(train_data, nres, ness, False)  \n",
    "X_cat, cat_features_sparse = vect_cat(train_data, cat_features, False)\n",
    "\n",
    "#Feature reduction for categorical columns (optional)\n",
    "#X_cat, cat_features = feat_reduce(X_cat, cat_features, labels, pcat)\n",
    "\n",
    "#Define joint sparse matrix\n",
    "X_vect = hstack([X_text, X_cat], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ce003370-854a-4638-8cef-8a1697b21826",
    "_uuid": "eca6466f32446384534cdb7835be464445264140"
   },
   "source": [
    " ## 3.4. Binary Classification using LightGBM\n",
    "Classification split train_data into a training set and a cross-validation set. Multiple folds are used. LightGBM is a gradient-boosted, tree-based classifier that uses histograms and binning to speed up classification and reduce memory. There is a real possibility of over-fitting using any tree-based classifier, especially with large training datasets; AUC was used here to evaluate validation fits. Extensive hyperparameter tuning was performed to achieve fast, accurate, and generalizable classification using several parameters:\n",
    "* limited number of features per vectorized text column (as discussed above)\n",
    "* allowing many boosting rounds (num_boost_round=100's-1000's)\n",
    "* shallow trees (max_depth~3-6)\n",
    "* #leaves/tree = 31)\n",
    "* increasing learning_rate  (currently 0.1-1)\n",
    "* allowing early stopping (early_stopping_rounds=20)\n",
    "* many other parameters (see code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e089ddc-bf07-4e18-a51f-a9793514befa",
    "_uuid": "e8cade910381784cb8125f5bb6194aaff6f56f91",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Straight-forward lightGBM classifier without Gridsearch\n",
    "def classify_gbm(X_vect, labels, cat_features, text_features):\n",
    "\n",
    "    t0 = time.time()\n",
    "    eval_results = {}\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vect, labels, test_size=0.2, random_state=0)\n",
    "    feature_names = text_features + cat_features\n",
    "\n",
    "    params = {'boosting_type': 'gbdt','objective': 'binary','metric': 'auc', 'max_depth': 12,\n",
    "              'num_leaves': 31,'feature_fraction': 0.85,'bagging_fraction': 0.85,'learning_rates':1,\n",
    "              'bagging_freq': 5,'verbose': 0,'num_threads': 1,'lambda_l2': 2,'min_gain_to_split': 0,\n",
    "              'min_data_in_leaf':50, 'num_boost_round':1000, 'early_stopping_rounds':20}  \n",
    "\n",
    "    clfGBM = gbm.train(params,\n",
    "                      gbm.Dataset(X_train, y_train),\n",
    "                      valid_sets=[gbm.Dataset(X_test, y_test)],\n",
    "                      categorical_feature = cat_features,\n",
    "                      verbose_eval = 100,\n",
    "                      evals_result = eval_results,\n",
    "                      feature_name = feature_names)\n",
    "\n",
    "    y_pred = clfGBM.predict(X_test, num_iteration=clfGBM.best_iteration)\n",
    "    y_pred_class = np.digitize(y_pred, [0.75])\n",
    "    #confusion = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "    print('\\nClassification:')\n",
    "    print('   Time (sec.):', round(time.time() - t0,1))\n",
    "    print('   Bin Counts:', np.bincount(y_pred_class))\n",
    "    print('   Best AUC:', round(roc_auc_score(y_test, y_pred), 3))\n",
    "    print('   Prediction Accuracy:', round(accuracy_score(y_test, y_pred_class), 3))\n",
    "    print('   F1 Score:', round(f1_score(y_test, y_pred_class), 3))\n",
    "    print('   Classification Report:\\n', classification_report(y_test, y_pred_class))\n",
    "    gbm.plot_metric(eval_results, metric='auc', figsize=(12,6), title='AUC metric: training validation set')\n",
    "    #print('Confusion Matrix Results:\\n#TP:',confusion[1,1],'\\n#TN:',confusion[0,0],'\\n#FP:',confusion[0,1],'\\n#FN:',confusion[1,0])\n",
    "\n",
    "    return clfGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "f9638e91ea10ba063330b0545a3c7eb314e745d4"
   },
   "outputs": [],
   "source": [
    "labels = train_data['project_is_approved']\n",
    "clf = classify_gbm(X_vect, labels, cat_features, text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15e7a5ae-cb22-4e47-b585-ae9805853f9f",
    "_uuid": "c168adfe502d6c3b4af8a2e5c993e201898a31c6",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ALTERNATE LightGBM classifier with grid search and kfold validation\n",
    "def classify_gbm_grid(X_vect, labels, cat_features, text_features):\n",
    "    t0 = time.time()\n",
    "    kfolds = 5\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vect, labels, test_size=0.3, random_state=0)\n",
    "    param_grid = {'learning_rate': [1], 'num_boost_round': [1000], 'max_depth': [6], 'reg_lambda': [1]}\n",
    "    clfGBM = gbm.LGBMClassifier(\n",
    "          boosting_type= 'gbdt', \n",
    "          objective = 'binary', \n",
    "          max_bin = 10, \n",
    "          silent = False,\n",
    "          num_leaves = 31,\n",
    "          min_split_gain = 0.0,\n",
    "          is_unbalance = True) #because classes are unbalanced\n",
    "          #evals_result_ = eval_results)\n",
    "\n",
    "    #Perform grid search using defined parameter grid, scoring, and #kfolds\n",
    "    grid_search = GridSearchCV(clfGBM, param_grid, scoring='roc_auc', verbose=1, cv=kfolds)\n",
    "    all_features = text_features + cat_features\n",
    "    grid_search.fit(X_train, y_train, \n",
    "                categorical_feature=cat_features, \n",
    "                feature_name=text_features + cat_features)\n",
    "\n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)\n",
    "    feature_importance = model.feature_importances_\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    display(results)\n",
    "\n",
    "    print('\\nClassification:')\n",
    "    print('   Time (sec.):', round(time.time() - t0,1))\n",
    "    print('   Test Score: {:.2f}'.format(model.score(X_test, y_test)))\n",
    "    print('   Best Params: {}'.format(grid_search.best_params_))\n",
    "    print('   Best Validation Score: {:.2f}'.format(grid_search.best_score_))\n",
    "    print('   Best Estimator:\\n{}'.format(model))\n",
    "    print('\\n   Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "46fc5dad-ceee-48cf-b8c1-f9b348cb2d29",
    "_uuid": "44c39799b4c740e408a241d93d13e1d4f630075d"
   },
   "source": [
    "The most important features in the model are shown below. Note that *Categorical features* start with a capital letter, whereas *text features* do not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "59b1989e-a6eb-4859-943c-70bfecbbcc44",
    "_uuid": "b64ce724f2902e0573efc842a2ed374937a4e17f"
   },
   "outputs": [],
   "source": [
    "#Show most important features\n",
    "features_df = pd.DataFrame(clf.feature_name(), columns=['feature'])\n",
    "features_df['importance'] = clf.feature_importance()\n",
    "features_df.sort_values('importance', ascending=False, inplace=True)\n",
    "features_df.set_index('feature', inplace=True)\n",
    "\n",
    "ax = features_df[:25].plot(kind='barh', figsize=(14,8), legend=False)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Model Importance', fontsize=16)\n",
    "ax.set_ylabel('Feature', fontsize=16)\n",
    "ax.set_title('25 Most Import Model Features', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a9cd3b98-8f10-49a8-a84c-34ffe39a001a",
    "_uuid": "3d4bd39da67eb75a035dad24327acb6daaa43778"
   },
   "source": [
    "# 4. Prediction: Apply Model to Test Data and Submit\n",
    "We will now use the trained model to make predictions on the actual provided test data. We then prepare a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_cell_guid": "c00726fb-e967-4004-81d4-9fa94136d816",
    "_uuid": "246d4afb22950d620adf7774649b76a5d42f64c3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use clf model to make predictions from test data; use same cat_features as above\n",
    "#del [train_data]\n",
    "test_data = pd.read_csv('../input/test.csv', sep=',')\n",
    "test_data = wrangle(test_data, resources)\n",
    "display(test_data.head(3))\n",
    "\n",
    "cat_features = ['Month','Grade_cat', '#Prior_cat','Subject_cat', \n",
    "                '$Total_cat', 'Teacher_prefix','#Project_categories']\n",
    "\n",
    "X_text, text_features = vect_text(test_data, nres, ness, False)\n",
    "X_cat, features = vect_cat(test_data, cat_features, False)\n",
    "X_vect = hstack([X_text, X_cat], 'csr')\n",
    "y_pred = clf.predict(X_vect)\n",
    "my_submission = pd.DataFrame({'id': test_data[\"id\"], 'project_is_approved': y_pred}) #y_pred[:,1]})\n",
    "print(test_data.shape,'\\n',test_data.columns)\n",
    "print(my_submission.head().to_csv(index=False))\n",
    "my_submission.to_csv('my_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a680385f-ec33-48f1-b1d2-5fcd837ebfb1",
    "_uuid": "3bff389494afa1aaf828d283c7d1b939d6a2f81c",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
